{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-825 Assignment 5: \n",
    "Shreya Sharma (ssharma5) \n",
    "<img align=\"right\" src=\"data/four.png\"  width=\"100\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Classification Model (40 points)\n",
    "Deliverables: On your website,\n",
    "\n",
    "* Report the test accuracy of your best model.\n",
    "\n",
    "* Visualize a few random test point clouds and mention the predicted classes for each. Also, visualize at least 1 failure prediction for each class (chair, vase and lamp), and provide interpretation in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Test accuracy of my best model = 94.02%  \n",
    "  \n",
    "  \n",
    "Correct Prediction cases with point could visualisation -\n",
    "\n",
    "| Point Cloud             |  Ground Truth | Prediction |\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "|![](./data/pc_cls_1.gif)|  Chair | Chair\n",
    "|![](./data/pc_cls_2.gif)|  Chair | Chair\n",
    "|![](./data/pc_cls_4.gif)|  Chair | Chair\n",
    "|![](./data/pc_cls_618.gif)|  Vase | Vase\n",
    "|![](./data/pc_cls_620.gif)|  Vase | Vase\n",
    "|![](./data/pc_cls_625.gif)|  Vase | Vase\n",
    "|![](./data/pc_cls_721.gif)|  Lamp | Lamp\n",
    "|![](./data/pc_cls_722.gif)|  Lamp | Lamp\n",
    "|![](./data/pc_cls_723.gif)|  Lamp | Lamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total correct predictions =  896    \n",
    "Total incorrect prediction =  57  \n",
    "\n",
    "Incorrect Prediction cases with point could visualisation -\n",
    "\n",
    "| Point Cloud             |  Ground Truth | Prediction |\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "|![](./data/pc_w_cls_290.gif)|  Chair | Lamp\n",
    "|![](./data/pc_w_cls_445.gif)|  Chair | Lamp\n",
    "|![](./data/pc_w_cls_619_2.gif)|  Vase | Lamp\n",
    "|![](./data/pc_w_cls_622_2.gif)|  Vase | Lamp\n",
    "|![](./data/pc_w_cls_726.gif)|  Lamp | Vase\n",
    "|![](./data/pc_w_cls_806.gif)|  Lamp | Vase\n",
    "|![](./data/pc_w_cls_864_0.gif)|  Lamp | Chair\n",
    "|![](./data/pc_w_cls_869_0.gif)|  Lamp | Chair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**  \n",
    "We see that the model learns to classify chairs pretty well and then vase and finally lamps. This is probably because the training data is not balanced across classes with many chair examples and lesser examples for vase and lamp. Also there seems much more diversity in the shapes of vase and lamps than there is in chairs. Thus, we see that the model often confuses unique looking lamps as vase or chair because it hasn't learnt the diverse distribution for lamp class very well. It also confuses weird shaped chair and vases as lamps which is expected as some of these shapes are too weird to be classified correctly even by a human merely from their point clouds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "  \n",
    "##  Q2. Segmentation Model (40 points)  \n",
    "\n",
    "Deliverables: On your website\n",
    "\n",
    "* Report the test accuracy of your best model.\n",
    "\n",
    "* Visualize segmentation results of at least 5 objects (including 2 bad predictions) with corresponding ground truth, report the prediction accuracy for each object, and provide interpretation in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Test accuracy of my best model = 87.82%  \n",
    "  \n",
    "  \n",
    "Correct Prediction cases with point could visualisation -\n",
    "\n",
    "Serial No. | Ground Truth Segmentation     | Predicted Segmentation  | Accuracy |\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "1 |![](./data/gt_seg_exp_0.gif)|  ![](./data/pred_seg_exp_0.gif) | 94.61%\n",
    "2 |![](./data/gt_seg_exp_1.gif)|  ![](./data/pred_seg_exp_1.gif) | 88.30%\n",
    "3 |![](./data/gt_seg_exp_2.gif)|  ![](./data/pred_seg_exp_2.gif) | 85.14%\n",
    "4 |![](./data/gt_seg_exp_6.gif)|  ![](./data/pred_seg_exp_6.gif) | 97.69%\n",
    "5 |![](./data/gt_seg_exp_7.gif)|  ![](./data/pred_seg_exp_7.gif) | 98.40%\n",
    "6 |![](./data/gt_seg_exp_235.gif)|  ![](./data/pred_seg_exp_235.gif) | 49.70%\n",
    "7 |![](./data/gt_seg_exp_238.gif)|  ![](./data/pred_seg_exp_238.gif) | 48.22%\n",
    "8 |![](./data/gt_seg_exp_351.gif)|  ![](./data/pred_seg_exp_351.gif) | 45.18%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**  \n",
    "PointNet architecture seems to work well on segmentation task of chairs in general. We see an accuracy close to 100% in many chair examples in the test dataset, some of which are also visualised in the table above. However,  the model seems to have learned the bias of predicting classes for points based on their relative location in the point cloud. For instance, the model tries to predict chair legs (dark blue) class for most of the points which are in the lower part of the point cloud even if the chair is a different kind which doesn't have any conventional legs, for instance the last example in the table above where the model fails to perform well. Also, in case of weird structured chairs the model seems to confuse till what part does the seat span (red) and where are the arms etc in the chair (see example 7 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## Q3. Robustness Analysis (20 points)\n",
    "Deliverables: On your website, for each experiment\n",
    "\n",
    "* Describe your procedure\n",
    "* For each task, report test accuracy and visualization on a few samples, in comparison with your results from Q1 & Q2.\n",
    "* Provide some interpretation in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. \n",
    "### Part1 : Test of invarience on Rotation\n",
    "Below table shows the PointNet performance on test pointclouds rotated at different angles along z axis for classification and segmentation problems.  \n",
    "\n",
    "**Procedure:**  For this experiment I simply applied a rotation tranformation of the 3D points in each of the point clouds in the test dataset and evaluated the performance of the model on the new rotated data set.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Angle    | Data 1  | Data 3 | Data 620 | Data 625 | Data 720 | Data 721 | Test Accuracy |\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|0 | ![](./data/rotate_cls_0_1_0.0_0.gif)| ![](./data/rotate_cls_0_3_0.0_0.gif)| ![](./data/rotate_cls_0_620_1.0_1.gif)| ![](./data/rotate_cls_0_625_1.0_1.gif)| ![](./data/rotate_cls_0_720_2.0_2.gif)| ![](./data/rotate_cls_0_721_2.0_2.gif)| 94.02%\n",
    "|Predicted Label | Chair | Chair | Vase | Vase | Lamp | Lamp\n",
    "|30 | ![](./data/rotate_cls_30_1_0.0_2.gif)| ![](./data/rotate_cls_30_3_0.0_0.gif)| ![](./data/rotate_cls_30_620_1.0_0.gif)| ![](./data/rotate_cls_30_625_1.0_1.gif)| ![](./data/rotate_cls_30_720_2.0_2.gif)| ![](./data/rotate_cls_30_721_2.0_0.gif)| 82.58%\n",
    "|Predicted Label | <span style=\"color:red\">Lamp</span>  | Chair | <span style=\"color:red\">Chair</span>  | Vase | Lamp | <span style=\"color:red\">Chair</span> \n",
    "|45 | ![](./data/rotate_cls_45_1_0.0_2.gif)| ![](./data/rotate_cls_45_3_0.0_1.gif)| ![](./data/rotate_cls_45_620_1.0_0.gif)| ![](./data/rotate_cls_45_625_1.0_1.gif)| ![](./data/rotate_cls_45_720_2.0_2.gif)| ![](./data/rotate_cls_45_721_2.0_2.gif)| 71.14%\n",
    "|Predicted Label | <span style=\"color:red\">Lamp</span>  |  <span style=\"color:red\">Vase</span> | <span style=\"color:red\">Chair</span>  | Vase | Lamp | Lamp\n",
    "|75 | ![](./data/rotate_cls_75_1_0.0_2.gif)| ![](./data/rotate_cls_75_3_0.0_1.gif)| ![](./data/rotate_cls_75_620_1.0_0.gif)| ![](./data/rotate_cls_75_625_1.0_2.gif)| ![](./data/rotate_cls_75_720_2.0_2.gif)| ![](./data/rotate_cls_45_721_2.0_2.gif)| 38.09%\n",
    "|Predicted Label | <span style=\"color:red\">Lamp</span>  |  <span style=\"color:red\">Vase</span> | <span style=\"color:red\">Chair</span>  | <span style=\"color:red\">Lamp</span> | Lamp | Lamp\n",
    "|90 | ![](./data/rotate_cls_90_1_0.0_2.gif)| ![](./data/rotate_cls_90_3_0.0_1.gif)| ![](./data/rotate_cls_90_620_1.0_0.gif)| ![](./data/rotate_cls_90_625_1.0_0.gif)| ![](./data/rotate_cls_90_720_2.0_2.gif)| ![](./data/rotate_cls_90_721_2.0_2.gif)| 29.38%\n",
    "|Predicted Label | <span style=\"color:red\">Lamp</span>  |  <span style=\"color:red\">Vase</span> | <span style=\"color:red\">Chair</span>  | <span style=\"color:red\">Chair</span> | Lamp | Lamp\n",
    "|120 | ![](./data/rotate_cls_120_1_0.0_1.gif)| ![](./data/rotate_cls_120_3_0.0_1.gif)| ![](./data/rotate_cls_120_620_1.0_0.gif)| ![](./data/rotate_cls_120_625_1.0_0.gif)| ![](./data/rotate_cls_120_720_2.0_2.gif)| ![](./data/rotate_cls_120_721_2.0_2.gif)| 30.53%\n",
    "|Predicted Label | <span style=\"color:red\">Vase</span>  |  <span style=\"color:red\">Vase</span> | <span style=\"color:red\">Chair</span>  | <span style=\"color:red\">Chair</span> | Lamp | Lamp\n",
    "|180 | ![](./data/rotate_cls_180_1_0.0_0.gif)| ![](./data/rotate_cls_180_3_0.0_1.gif)| ![](./data/rotate_cls_180_620_1.0_1.gif)| ![](./data/rotate_cls_180_625_1.0_0.gif)| ![](./data/rotate_cls_180_720_2.0_2.gif)| ![](./data/rotate_cls_180_721_2.0_2.gif)| 47.11%\n",
    "|Predicted Label | Chair |  <span style=\"color:red\">Vase</span> | Vase | <span style=\"color:red\">Chair</span> | Lamp | Lamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/cls_rot.png\" alt=\"My Image\" width=\"400\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**   \n",
    "From the table above, you can see that on the original test set the accuracy is around 94% which drops significantly as we increase the angle of rotation from 82.6% at 30 deg rotation to a lowest of 29.4% at 90 deg rotation where it confuses chairs with lamp or vase and misclassifies many objects. But the wrong predictions that the model makes in certain cases do make sense if we see visually. For example, at 90 deg rotation the first vase does look like a couch and the second vase does look a conical lamp shade and the model also classifies it as chair and lamp respectively which can be expected from it. Thus, we can conclude that the network is not really robust to rotation and thereâ€™s a significant drop in classification accuracy even if we rotate the point clouds by few degrees (along the z-axis). This is because it has been trained only on upright images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 0 degrees | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "| Ground Truth | ![](./data/rotate_gt_seg_0_1.gif)| ![](./data/rotate_gt_seg_0_2.gif)| ![](./data/rotate_gt_seg_0_6.gif)| ![](./data/rotate_gt_seg_0_7.gif)| ![](./data/rotate_gt_seg_0_351.gif)\n",
    "| Predicted | ![](./data/rotate_pred_seg_0_1.gif)| ![](./data/rotate_pred_seg_0_2.gif)| ![](./data/rotate_pred_seg_0_6.gif)| ![](./data/rotate_pred_seg_0_7.gif)| ![](./data/rotate_pred_seg_0_351.gif)\n",
    "| Accuracy (88.27%) | 98.1% | 86.01% | 97.82% | 97.2% | 58.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 45 degrees | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/rotate_gt_seg_45_1.gif)| ![](./data/rotate_gt_seg_45_2.gif)| ![](./data/rotate_gt_seg_45_6.gif)| ![](./data/rotate_gt_seg_45_7.gif)| ![](./data/rotate_gt_seg_45_351.gif)\n",
    "|Predicted | ![](./data/rotate_pred_seg_45_1.gif)| ![](./data/rotate_pred_seg_45_2.gif)| ![](./data/rotate_pred_seg_45_6.gif)| ![](./data/rotate_pred_seg_45_7.gif)| ![](./data/rotate_pred_seg_45_351.gif)\n",
    "|Accuracy (61.54%) | 54.86% | 36.36% | 81.12% | 72.22% | 32.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 90 degrees | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/rotate_gt_seg_90_1.gif)| ![](./data/rotate_gt_seg_90_2.gif)| ![](./data/rotate_gt_seg_90_6.gif)| ![](./data/rotate_gt_seg_90_7.gif)| ![](./data/rotate_gt_seg_90_351.gif)\n",
    "|Predicted | ![](./data/rotate_pred_seg_90_1.gif)| ![](./data/rotate_pred_seg_90_2.gif)| ![](./data/rotate_pred_seg_90_6.gif)| ![](./data/rotate_pred_seg_90_7.gif)| ![](./data/rotate_pred_seg_90_351.gif)\n",
    "|Accuracy (45.74%) | 59.87% | 43.06% | 39.86% | 36.68% | 41.78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 180 degrees | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/rotate_gt_seg_180_1.gif)| ![](./data/rotate_gt_seg_180_2.gif)| ![](./data/rotate_gt_seg_180_6.gif)| ![](./data/rotate_gt_seg_180_7.gif)| ![](./data/rotate_gt_seg_180_351.gif)\n",
    "|Predicted | ![](./data/rotate_pred_seg_180_1.gif)| ![](./data/rotate_pred_seg_180_2.gif)| ![](./data/rotate_pred_seg_180_6.gif)| ![](./data/rotate_pred_seg_180_7.gif)| ![](./data/rotate_pred_seg_180_351.gif)\n",
    "|Accuracy (39.09%) | 53.87% | 24.76% | 34.60% | 29.96% | 66.59%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/seg_rot.png\" alt=\"My Image\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**   \n",
    "From the table above, you can see that on the original test set the segmetation accuracy is around 88% which drops significantly as we increase the angle of rotation from 61.5% at 30 deg rotation to as low as 39%at 180 deg rotation where it confuses legs of the chairs turned upside down with the backrest. This was expected because the model was solely trained on upright images and so it kind of leanrt the inherent bias that the blue part of the chairs (the legs) is in the lower section of points of the point cloud and bakc rest is in the upper section of the point cloud. However we see that it still manages to predict the red section (the seat) well even on rotation around z-axis because the spacial location of the seat points has not changed much even though their oritentation might have changed with rotation. But can say that this model on segmentation is not robust to rotation at all given the huge drop in performance even on slight rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Test of invarience on number of points\n",
    "Below table shows the PointNet performance on test pointclouds by sampling different number of points from the point cloud.  \n",
    "\n",
    "**Procedure:**  For this experiment I simply changed the num_points attribute to sample a subset of points randomly from the point clouds of the test dataset and evaluated the performance of the model on the new data set for classification and segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Num Points    | Data 1  | Data 3 | Data 620 | Data 625 | Data 720 | Data 721 | Test Accuracy |\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|10000 | ![](./data/nsample_cls_10000_1_0.0_0.gif)| ![](./data/nsample_cls_10000_3_0.0_0.gif)| ![](./data/nsample_cls_10000_620_1.0_1.gif)| ![](./data/nsample_cls_10000_625_1.0_1.gif)| ![](./data/nsample_cls_10000_720_2.0_2.gif)| ![](./data/nsample_cls_10000_721_2.0_2.gif)| 94.02%\n",
    "|Predicted Label | Chair | Chair | Vase | Vase | Lamp | Lamp\n",
    "|5000 | ![](./data/nsample_cls_5000_1_0.0_0.gif)| ![](./data/nsample_cls_5000_3_0.0_0.gif)| ![](./data/nsample_cls_5000_620_1.0_1.gif)| ![](./data/nsample_cls_5000_625_1.0_1.gif)| ![](./data/nsample_cls_5000_720_2.0_2.gif)| ![](./data/nsample_cls_5000_721_2.0_2.gif)| 93.8%\n",
    "|Predicted Label | Chair | Chair | Vase | Vase | Lamp | Lamp\n",
    "|1000 | ![](./data/nsample_cls_1000_1_0.0_0.gif)| ![](./data/nsample_cls_1000_3_0.0_0.gif)| ![](./data/nsample_cls_1000_620_1.0_1.gif)| ![](./data/nsample_cls_1000_625_1.0_1.gif)| ![](./data/nsample_cls_1000_720_2.0_2.gif)| ![](./data/nsample_cls_1000_721_2.0_2.gif)| 94.2%\n",
    "|Predicted Label | Chair | Chair | Vase | Vase | Lamp | Lamp\n",
    "|500 | ![](./data/nsample_cls_500_1_0.0_0.gif)| ![](./data/nsample_cls_500_3_0.0_0.gif)| ![](./data/nsample_cls_500_620_1.0_1.gif)| ![](./data/nsample_cls_500_625_1.0_1.gif)| ![](./data/nsample_cls_500_720_2.0_2.gif)| ![](./data/nsample_cls_500_721_2.0_2.gif)| 93.07%\n",
    "|Predicted Label | Chair | Chair | Vase | Vase | Lamp | Lamp\n",
    "|100 | ![](./data/nsample_cls_100_1_0.0_0.gif)| ![](./data/nsample_cls_100_3_0.0_0.gif)| ![](./data/nsample_cls_100_620_1.0_2.gif)| ![](./data/nsample_cls_100_625_1.0_1.gif)| ![](./data/nsample_cls_100_720_2.0_2.gif)| ![](./data/nsample_cls_100_721_2.0_2.gif)| 87.51%\n",
    "|Predicted Label | Chair | Chair | <span style=\"color:red\">Lamp</span> | Vase | Lamp | Lamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/cls_nsample.png\" alt=\"My Image\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretaion**:  \n",
    "The model seems pretty robust to number of points uptill 100. The performance is more or less same. But as we decrease the number of sampled points to less than 100 we are more likely to see incorrect classification because the shapes with such sparse poitn clouds will be very diffcult to identify and even more difficult to distinguish. In general, we can say that this implementation of PointNet is robust to number of points upto a certain limit and fails to perform beyond points < 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 5000 Points | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/nsample_gt_seg_5000_1.gif)| ![](./data/nsample_gt_seg_5000_2.gif)| ![](./data/nsample_gt_seg_5000_6.gif)| ![](./data/nsample_gt_seg_5000_7.gif)| ![](./data/nsample_gt_seg_5000_351.gif)\n",
    "|Predicted | ![](./data/nsample_pred_seg_5000_1.gif)| ![](./data/nsample_pred_seg_5000_2.gif)| ![](./data/nsample_pred_seg_5000_6.gif)| ![](./data/nsample_pred_seg_5000_7.gif)| ![](./data/nsample_pred_seg_5000_351.gif)\n",
    "|Accuracy (88.20%) | 98.1% | 86.08% | 97.8% | 97.5% | 57.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 1000 Points | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/nsample_gt_seg_1000_1.gif)| ![](./data/nsample_gt_seg_1000_2.gif)| ![](./data/nsample_gt_seg_1000_6.gif)| ![](./data/nsample_gt_seg_1000_7.gif)| ![](./data/nsample_gt_seg_1000_351.gif)\n",
    "|Predicted | ![](./data/nsample_pred_seg_1000_1.gif)| ![](./data/nsample_pred_seg_1000_2.gif)| ![](./data/nsample_pred_seg_1000_6.gif)| ![](./data/nsample_pred_seg_1000_7.gif)| ![](./data/nsample_pred_seg_1000_351.gif)\n",
    "|Accuracy (87.3%) | 98.1% | 88.1% | 98.1% | 97.3% | 48.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 500 Points | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/nsample_gt_seg_500_1.gif)| ![](./data/nsample_gt_seg_500_2.gif)| ![](./data/nsample_gt_seg_500_6.gif)| ![](./data/nsample_gt_seg_500_7.gif)| ![](./data/nsample_gt_seg_500_351.gif)\n",
    "|Predicted | ![](./data/nsample_pred_seg_500_1.gif)| ![](./data/nsample_pred_seg_500_2.gif)| ![](./data/nsample_pred_seg_500_6.gif)| ![](./data/nsample_pred_seg_500_7.gif)| ![](./data/nsample_pred_seg_500_351.gif)\n",
    "|Accuracy (85.97%) | 99.1% | 86.1% | 97.1% | 97.2% | 48.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 100 Points | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/nsample_gt_seg_100_1.gif)| ![](./data/nsample_gt_seg_100_2.gif)| ![](./data/nsample_gt_seg_100_6.gif)| ![](./data/nsample_gt_seg_100_7.gif)| ![](./data/nsample_gt_seg_100_351.gif)\n",
    "|Predicted | ![](./data/nsample_pred_seg_100_1.gif)| ![](./data/nsample_pred_seg_100_2.gif)| ![](./data/nsample_pred_seg_100_6.gif)| ![](./data/nsample_pred_seg_100_7.gif)| ![](./data/nsample_pred_seg_100_351.gif)\n",
    "|Accuracy (79.97%) | 93.1% | 80.1% | 96.1% | 97.0% | 50.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 50 Points | Data 1  | Data 2 | Data 6 | Data 7 | Data 351 | \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|Ground Truth | ![](./data/nsample_gt_seg_50_1.gif)| ![](./data/nsample_gt_seg_50_2.gif)| ![](./data/nsample_gt_seg_50_6.gif)| ![](./data/nsample_gt_seg_50_7.gif)| ![](./data/nsample_gt_seg_50_351.gif)\n",
    "|Predicted | ![](./data/nsample_pred_seg_50_1.gif)| ![](./data/nsample_pred_seg_50_2.gif)| ![](./data/nsample_pred_seg_50_6.gif)| ![](./data/nsample_pred_seg_50_7.gif)| ![](./data/nsample_pred_seg_50_351.gif)\n",
    "|Accuracy (37.68%) | 60.1% | 74.01% | 90.8% | 90.2% | 40.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/seg_nsample.png\" alt=\"My Image\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "As you can see that even segmentation task is pretty robust to the number of points in the point cloud up to a certain level. If the number of points become lesser than 100 then we see a significant drop in performnce from 88% in 10k points to 80% with 100 points to 38% with 50 points. Thus, we can say that the model to robust to number of sampled points only upto a good limit but performs poorly in case of very low sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![](./data/gt_seg_exp_0.gif)|  ![](./data/pred_seg_exp_0.gif) | 94.61%\n",
    "![](./data/gt_seg_exp_1.gif)|  ![](./data/pred_seg_exp_1.gif) | 88.30%\n",
    "![](./data/gt_seg_exp_2.gif)|  ![](./data/pred_seg_exp_2.gif) | 85.14%\n",
    "![](./data/gt_seg_exp_6.gif)|  ![](./data/pred_seg_exp_6.gif) | 97.69%\n",
    "![](./data/gt_seg_exp_7.gif)|  ![](./data/pred_seg_exp_7.gif) | 98.40%\n",
    "![](./data/gt_seg_exp_235.gif)|  ![](./data/pred_seg_exp_235.gif) | 49.70%\n",
    "![](./data/gt_seg_exp_238.gif)|  ![](./data/pred_seg_exp_238.gif) | 48.22%\n",
    "![](./data/gt_seg_exp_351.gif)|  ![](./data/pred_seg_exp_351.gif) | 45.18%\n",
    "![](./data/gt_seg_exp_60.gif)|  ![](./data/pred_seg_exp_60.gif) | 45.91% -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## Q4. Expressive architectures (10 points + 20 bonus points)\n",
    "\n",
    "Deliverables: On your website,\n",
    "\n",
    "* Describe the model you have implemented.\n",
    "* For one of the tasks (either segmentation or classification), report the test accuracy of your best model, in comparison with your results from Q1 & Q2.\n",
    "* Visualize results in comparison to ones obtained in the earlier parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointNet++\n",
    "**Model Description**  \n",
    "I have implemented the pointNet++ architecture as explained in the paper and followed the implementation from - https://github.com/yanx27/Pointnet_Pointnet2_pytorch\n",
    "\n",
    "<img src=\"data/pointnet++.png\" alt=\"My Image\" width=\"600\" height=\"300\">\n",
    "\n",
    "The PointNet++ architecture is based on a hierarchical neural network that processes point clouds in a coarse-to-fine manner, with each level of the hierarchy capturing increasingly detailed information about the point cloud. My implementation consists of these key components:\n",
    "\n",
    "* PointNet Set Abstraction Layers: There are three point set abstraction layers in my implementation. These layers take a set of points as input and output a smaller set of representative points, along with their local neighborhoods. This is achieved through a hierarchical clustering process that groups nearby points together and computes a representative point for each group.\n",
    "\n",
    "* Fully Connected Layers: These layers take the features extracted from the previous layers and use them to classify the point cloud into different object classes. \n",
    " \n",
    "Test accuracy of the best model = 98.43% (compared to 94.02% in pointnet model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGCNN \n",
    "**Model Description**  \n",
    "I have implemented the DGCNN architecture as explained in the paper and followed the implementation from - https://github.com/antao97/dgcnn.pytorch\n",
    "\n",
    "<img src=\"data/dgcnn.png\" alt=\"My Image\" width=\"600\" height=\"300\">  \n",
    "\n",
    "The DGCNN architecture is based on a dynamic graph convolutional neural network that operates directly on the point cloud data. The key idea behind the architecture is to construct a graph representation of the point cloud, where each point is treated as a node in the graph and edges are defined between adjacent points.\n",
    "\n",
    "My DGCNN architecture consists of these key components:\n",
    "\n",
    "* Graph Construction: The input point cloud is transformed into a k-nearest neighbor graph, where each point is connected to its k nearest neighbors (I set it to 20 in these expeirments). The graph is represented as an adjacency matrix, where each entry corresponds to the weight of the edge between two points.\n",
    "\n",
    "* Edge Convolution: The edge convolution layer applies a convolution operation to the edges in the graph, which aggregates information from neighboring points. This operation is performed by computing a weighted sum of the features of the neighboring points, where the weights are determined by the edge weights in the adjacency matrix.\n",
    "\n",
    "* Local Pooling: The local pooling layer aggregates information from neighboring nodes by applying max-pooling to the features of the nodes within a fixed radius. This allows the network to capture local geometric patterns in the point cloud.\n",
    "\n",
    "* Edge Pooling: The edge pooling layer applies a pooling operation to the edges in the graph, which aggregates information from neighboring edges. This operation is performed by selecting the k most important edges based on their weights, and computing a new adjacency matrix that only includes those edges.\n",
    "\n",
    "* Fully Connected Layers: The output of the pooling layers is passed through several fully connected layers, which produce the final classification output.\n",
    " \n",
    "Test accuracy of the best model = 97.69% (compared to 94.02% in pointnet model)  \n",
    "  \n",
    "Below table shows the performance of PointNet++ and DGCNN models on some failure cases of basic PointNet Model. You can observe that these improved models are able to predict the right class for these difficult examples.\n",
    "\n",
    "| Point Cloud             |  Ground Truth | Prediction by PointNet (Q1) | Prediction by Pointnet++ | Prediction by DGCNN \n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
    "|![](./data/pc_w_cls_290.gif)|  Chair | Lamp | Chair | Chair\n",
    "|![](./data/pc_w_cls_726.gif)|  Lamp | Vase | Lamp | Lamp\n",
    "|![](./data/pc_w_cls_869_0.gif)|  Lamp | Chair | Lamp | Lamp\n",
    "|![](./data/pc_w_cls_619_2.gif)|  Vase | Lamp | Vase | Vase\n",
    "|![](./data/pc_w_cls_622_2.gif)|  Vase | Lamp | Vase | Vase"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1c5f0f4a8483e2f2e36295d8a810401dfd49b2caaabc50106f51573c6b0a590"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('l3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
